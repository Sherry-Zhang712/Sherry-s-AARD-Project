{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ff1497-3558-450b-a81d-7982ddd1f46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/innobridge/anaconda3/lib/python3.12/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/innobridge/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/innobridge/anaconda3/lib/python3.12/site-packages (from xgboost) (2.22.3)\n",
      "Requirement already satisfied: scipy in /home/innobridge/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91dcbb27-160a-4756-b009-47a52eaf655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(\"XGBoost imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84f4749-19a9-4ba7-92c3-858bdd2978cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 41472 candidates, totalling 124416 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[1;32m     58\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[1;32m     62\u001b[0m best_xgb_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "data_sampled = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [700, 800],             # Number of boosted trees to fit\n",
    "    'max_depth': [15, 20],                  # Maximum depth of a tree\n",
    "    'learning_rate': [0.01, 0.03],          # Step size shrinkage\n",
    "    'subsample': [0.6, 0.8, 1],             # Subsample ratio of the training instance\n",
    "    'colsample_bytree': [0.5, 0.6],         # Subsample ratio of columns when constructing each tree\n",
    "    'colsample_bylevel': [0.6, 0.8, 1],     # Subsample ratio of columns for each split, in each level\n",
    "    'colsample_bynode': [0.6, 0.8, 1],      # Subsample ratio of columns for each split, in each node\n",
    "    'min_child_weight': [3, 5],             # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'gamma': [0.1],                         # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    'reg_alpha': [0, 0.1, 1, 10],           # L1 regularization term on weights\n",
    "    'reg_lambda': [1, 1.5, 2, 5],           # L2 regularization term on weights\n",
    "    'scale_pos_weight': [1, 3, 5],          # Balancing of positive and negative weights\n",
    "    'tree_method': ['hist'],                # Use histogram-based method for training\n",
    "    'device': ['cuda']                      # Specify the GPU device\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29317780-11a0-4fc5-955d-4423868b8a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 10368 candidates, totalling 31104 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[1;32m     56\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[1;32m     60\u001b[0m best_xgb_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "data_sampled = data.sample(n=10000, random_state=42)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.03],          # Step size shrinkage\n",
    "    'subsample': [0.6, 0.8, 1],             # Subsample ratio of the training instance\n",
    "    'colsample_bytree': [0.5, 0.6],         # Subsample ratio of columns when constructing each tree\n",
    "    'colsample_bylevel': [0.6, 0.8, 1],     # Subsample ratio of columns for each split, in each level\n",
    "    'colsample_bynode': [0.6, 0.8, 1],      # Subsample ratio of columns for each split, in each node\n",
    "    'min_child_weight': [3, 5],             # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'gamma': [0.1],                         # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    'reg_alpha': [0, 0.1, 1, 10],           # L1 regularization term on weights\n",
    "    'reg_lambda': [1, 1.5, 2, 5],           # L2 regularization term on weights\n",
    "    'scale_pos_weight': [1, 3, 5],          # Balancing of positive and negative weights\n",
    "    'tree_method': ['hist'],                # Use histogram-based method for training\n",
    "    'device': ['cuda']                      # Specify the GPU device\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe39195d-e925-4fc4-96f9-d3d685427003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters found:  {'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8, 'device': 'cuda', 'gamma': 0.2, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 182, 'reg_alpha': 10, 'reg_lambda': 5, 'scale_pos_weight': 1, 'subsample': 0.75, 'tree_method': 'hist'}\n",
      "Accuracy: 0.8145347023966605\n",
      "Precision: 0.8308713608837763\n",
      "Recall: 0.7886768401938502\n",
      "F1 Score: 0.8092244479952619\n",
      "Confusion Matrix: \n",
      "[[70225 13350]\n",
      " [17573 65584]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [182],             \n",
    "    'max_depth': [6], \n",
    "    'learning_rate': [0.2],          \n",
    "    'subsample': [0.75],             \n",
    "    'tree_method': ['hist'],  # Use GPU for training\n",
    "    'device': ['cuda'],  # Set device to CUDA for GPU training\n",
    "    'colsample_bytree': [0.8],        \n",
    "    'colsample_bylevel': [1],\n",
    "    'colsample_bynode': [1],      \n",
    "    'min_child_weight': [3],  \n",
    "    'gamma': [0.2],  # Minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "    'reg_alpha': [10],  # L1 regularization term on weights\n",
    "    'reg_lambda': [5, 6, 7],  # L2 regularization term on weights\n",
    "    'scale_pos_weight': [0.5, 1],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c82d7f4-9d53-4c12-b93f-9a328b61c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters found:  {'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.8, 'gamma': 0.2, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 182, 'reg_alpha': 10, 'reg_lambda': 5, 'scale_pos_weight': 1, 'subsample': 0.75}\n",
      "Accuracy: 0.8144027541203848\n",
      "Precision: 0.8306063522617901\n",
      "Recall: 0.7887249419772238\n",
      "F1 Score: 0.809124049321186\n",
      "Confusion Matrix: \n",
      "[[70199 13376]\n",
      " [17569 65588]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [182],             \n",
    "    'max_depth': [6],   \n",
    "    'learning_rate': [0.2], \n",
    "    'subsample': [0.75],\n",
    "    'colsample_bylevel': [1], \n",
    "    'colsample_bytree': [0.8],\n",
    "    'colsample_bynode': [1],      \n",
    "    'min_child_weight': [3],\n",
    "     'gamma': [0.2],                        \n",
    "    'reg_alpha': [10], \n",
    "     'reg_lambda': [5],  \n",
    "    'scale_pos_weight': [1],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "042ae6ee-8155-4104-a83d-e55ffeb91606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters found:  {'max_depth': 6, 'n_estimators': 182}\n",
      "Accuracy: 0.8148225895448984\n",
      "Precision: 0.8306392451493765\n",
      "Recall: 0.7897350794280698\n",
      "F1 Score: 0.8096708770242697\n",
      "Confusion Matrix: \n",
      "[[70185 13390]\n",
      " [17485 65672]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [182],             \n",
    "    'max_depth': [6],   \n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e72cc7f0-c40e-49a2-aaf0-31f39da779ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "Best parameters found:  {'learning_rate': 0.2, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 182}\n",
      "Accuracy: 0.8149425425233309\n",
      "Precision: 0.8301977322659663\n",
      "Recall: 0.7906730642038553\n",
      "F1 Score: 0.809953496966524\n",
      "Confusion Matrix: \n",
      "[[70127 13448]\n",
      " [17407 65750]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [182, 200],             \n",
    "    'max_depth': [5, 6, 7],   \n",
    "    'learning_rate': [0.1, 0.2, 0.3],   \n",
    "    'min_child_weight': [2.5, 3, 4],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f94d3d37-24c0-4414-87c5-003493b4a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters found:  {'colsample_bylevel': 1.0, 'colsample_bynode': 1.0, 'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.2, 'max_delta_step': 0, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 182, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1.0}\n",
      "Accuracy: 0.8153203944053931\n",
      "Precision: 0.8310510943367598\n",
      "Recall: 0.7903964789494571\n",
      "F1 Score: 0.8102141193003217\n",
      "Confusion Matrix: \n",
      "[[70213 13362]\n",
      " [17430 65727]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [182],             \n",
    "    'max_depth': [7],   \n",
    "    'learning_rate': [0.2],   \n",
    "    'min_child_weight': [3],\n",
    "  'gamma': [0], \n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'colsample_bylevel': [1.0],\n",
    "    'colsample_bynode': [1.0],\n",
    "   'reg_alpha': [10],\n",
    "    'reg_lambda': [1],\n",
    "    'max_delta_step': [0],\n",
    "    'scale_pos_weight': [1]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0137d7e4-3507-48c8-b209-7fabc9e5f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Best parameters found:  {'colsample_bylevel': 1.0, 'colsample_bynode': 1.0, 'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 8, 'min_child_weight': 2, 'n_estimators': 500, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1.0}\n",
      "Accuracy: 0.8158421898615743\n",
      "Precision: 0.830718789407314\n",
      "Recall: 0.7921882703801243\n",
      "F1 Score: 0.810996140517183\n",
      "Confusion Matrix: \n",
      "[[70151 13424]\n",
      " [17281 65876]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 800],             \n",
    "    'max_depth': [7, 8, 9],   \n",
    "    'learning_rate': [0.1, 0.2],   \n",
    "    'min_child_weight': [2, 3],\n",
    "  'gamma': [0], \n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'colsample_bylevel': [1.0],\n",
    "    'colsample_bynode': [1.0],\n",
    "   'reg_alpha': [10],\n",
    "    'reg_lambda': [1],\n",
    "    'max_delta_step': [0],\n",
    "    'scale_pos_weight': [1]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee718f42-bc22-4f01-9ad0-d93f5f5a84e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/bin/python\n",
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/innobridge/anaconda3/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bylevel': 1.0, 'colsample_bynode': 1.0, 'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 500, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1.0}\n",
      "Accuracy: 0.8157522251277499\n",
      "Precision: 0.8304616897112391\n",
      "Recall: 0.7923325757302452\n",
      "F1 Score: 0.81094919259551\n",
      "Confusion Matrix: \n",
      "[[70124 13451]\n",
      " [17269 65888]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from xgboost\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_id.*')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, message='.*gpu_hist.*')\n",
    "\n",
    "# Load data\n",
    "file_path = r\"/home/innobridge/sherry/Final_Balanced_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define poverty threshold and create target variable\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "# Drop the PINCP column\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [500],             \n",
    "    'max_depth': [8],   \n",
    "    'learning_rate': [0.05, 0.1],   \n",
    "    'min_child_weight': [1],\n",
    "  'gamma': [0], \n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [1, 0.6, 0.4],\n",
    "    'colsample_bylevel': [1.0],\n",
    "    'colsample_bynode': [1.0],\n",
    "   'reg_alpha': [10],\n",
    "    'reg_lambda': [1],\n",
    "    'max_delta_step': [0],\n",
    "    'scale_pos_weight': [1]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier without use_label_encoder, using GPU\n",
    "xgb_model = xgb.XGBClassifier(random_state=18, eval_metric='logloss')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "precision = precision_score(Y_test, predictions)\n",
    "recall = recall_score(Y_test, predictions)\n",
    "f1 = f1_score(Y_test, predictions)\n",
    "conf_matrix = confusion_matrix(Y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2dc80-823e-4397-a3e3-702b6319a8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

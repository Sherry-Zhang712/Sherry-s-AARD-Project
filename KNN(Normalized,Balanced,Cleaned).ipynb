{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f2e041-1602-42b5-91aa-4b1d6d9bddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Merged Data:\n",
      "AGEP            0\n",
      "ANC1P           0\n",
      "CIT             0\n",
      "JWTRNS    1830018\n",
      "DIS             0\n",
      "SCHL        90621\n",
      "ESR        563988\n",
      "GCM       3351901\n",
      "HICOV           0\n",
      "HISP            0\n",
      "PINCP      522876\n",
      "OCCP      1366240\n",
      "ENG       2763995\n",
      "MAR             0\n",
      "MIG         29799\n",
      "MIL        604751\n",
      "RAC1P           0\n",
      "POBP            0\n",
      "SEX             0\n",
      "FOD1P     2456369\n",
      "DRATX     3132909\n",
      "dtype: int64\n",
      "Total number of rows before dropping rows for NaN PINCP: 3373378\n",
      "Number of rows dropped for NaN PINCP: 522876\n",
      "Total number of rows after dropping NaN PINCP rows: 2850502\n",
      "Number of rows dropped for NaN in any column: 857505\n",
      "Total number of rows after dropping rows for NaN in any column: 1992997\n",
      "Class distribution after balancing:\n",
      "poverty_status\n",
      "True     416830\n",
      "False    416830\n",
      "Name: count, dtype: int64\n",
      "The final balanced, normalized, and standardized data has been saved to: /Users/sherryzhang/Downloads/Final_Balanced_Data_Normalized_Standardized.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "file_path1 = '/Users/sherryzhang/Downloads/psam_pusa.csv'\n",
    "file_path2 = '/Users/sherryzhang/Downloads/psam_pusb.csv'\n",
    "\n",
    "data1 = pd.read_csv(file_path1)\n",
    "data2 = pd.read_csv(file_path2)\n",
    "\n",
    "columns_to_keep = ['AGEP', 'ANC1P', 'CIT', 'JWTRNS', 'DIS', 'SCHL', 'ESR', 'GCM', 'HICOV', 'HISP', 'PINCP', 'OCCP', 'ENG', 'MAR', 'MIG', 'MIL', 'RAC1P', 'POBP', 'SEX', 'FOD1P', 'DRATX']\n",
    "data1 = data1[columns_to_keep]\n",
    "data2 = data2[columns_to_keep]\n",
    "\n",
    "merged_data = pd.concat([data1, data2])\n",
    "\n",
    "print(\"Missing values in Merged Data:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "total_rows_before = merged_data.shape[0]\n",
    "print(f\"Total number of rows before dropping rows for NaN PINCP: {total_rows_before}\")\n",
    "\n",
    "initial_row_count = merged_data.shape[0]\n",
    "merged_data = merged_data.dropna(subset=['PINCP'])\n",
    "final_row_count = merged_data.shape[0]\n",
    "rows_dropped_for_PINCP = initial_row_count - final_row_count\n",
    "\n",
    "print(f\"Number of rows dropped for NaN PINCP: {rows_dropped_for_PINCP}\")\n",
    "print(f\"Total number of rows after dropping NaN PINCP rows: {final_row_count}\")\n",
    "\n",
    "columns_to_drop = ['JWTRNS', 'GCM', 'ENG', 'FOD1P', 'DRATX']\n",
    "merged_data = merged_data.drop(columns=columns_to_drop)\n",
    "\n",
    "initial_row_count = merged_data.shape[0]\n",
    "merged_data = merged_data.dropna()\n",
    "final_row_count = merged_data.shape[0]\n",
    "rows_dropped_for_any_nan = initial_row_count - final_row_count\n",
    "\n",
    "print(f\"Number of rows dropped for NaN in any column: {rows_dropped_for_any_nan}\")\n",
    "print(f\"Total number of rows after dropping rows for NaN in any column: {final_row_count}\")\n",
    "\n",
    "poverty_threshold = 13590\n",
    "merged_data['poverty_status'] = merged_data['PINCP'] <= poverty_threshold\n",
    "\n",
    "majority_class = merged_data[merged_data['poverty_status'] == False]\n",
    "minority_class = merged_data[merged_data['poverty_status'] == True]\n",
    "\n",
    "majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
    "balanced_data = pd.concat([minority_class, majority_downsampled])\n",
    "\n",
    "print(\"Class distribution after balancing:\")\n",
    "print(balanced_data['poverty_status'].value_counts())\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "columns_to_scale = ['AGEP', 'ANC1P', 'CIT', 'DIS', 'SCHL', 'ESR', 'HICOV', 'HISP', 'OCCP', 'MAR', 'MIG', 'MIL', 'RAC1P', 'POBP', 'SEX']\n",
    "balanced_data[columns_to_scale] = scaler.fit_transform(balanced_data[columns_to_scale])\n",
    "\n",
    "# Normalize\n",
    "min_max_scaler = MinMaxScaler()\n",
    "balanced_data[columns_to_scale] = min_max_scaler.fit_transform(balanced_data[columns_to_scale])\n",
    "\n",
    "output_file_path = '/Users/sherryzhang/Downloads/Final_Balanced_Data_Normalized_Standardized.csv'\n",
    "balanced_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"The final balanced, normalized, and standardized data has been saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d0996e-89bf-48dd-9469-d627151aec00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best parameters found:  {'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "Training Accuracy: 0.8219552935249382\n",
      "Training Precision: 0.8394367620768293\n",
      "Training Recall: 0.7964803864861706\n",
      "Training F1 Score: 0.8173945939462163\n",
      "Training Confusion Matrix: \n",
      "[[282421  50834]\n",
      " [ 67909 265764]]\n",
      "Testing Accuracy: 0.7932850322673513\n",
      "Testing Precision: 0.8080811915518267\n",
      "Testing Recall: 0.7679088952222903\n",
      "Testing F1 Score: 0.7874830435318781\n",
      "Testing Confusion Matrix: \n",
      "[[68409 15166]\n",
      " [19300 63857]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "file_path = r\"/Users/sherryzhang/Downloads/Final_Balanced_Data_Normalized_Standardized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "poverty_threshold = 13590\n",
    "data['poverty_status'] = np.where(data['PINCP'] <= poverty_threshold, 1, 0)\n",
    "\n",
    "data = data.drop(columns=['PINCP'])\n",
    "\n",
    "X = data.drop(columns=['poverty_status'])\n",
    "Y = data['poverty_status']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=18)\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "train_predictions = best_knn_model.predict(X_train)\n",
    "test_predictions = best_knn_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(Y_train, train_predictions)\n",
    "train_precision = precision_score(Y_train, train_predictions)\n",
    "train_recall = recall_score(Y_train, train_predictions)\n",
    "train_f1 = f1_score(Y_train, train_predictions)\n",
    "train_conf_matrix = confusion_matrix(Y_train, train_predictions)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Precision: {train_precision}\")\n",
    "print(f\"Training Recall: {train_recall}\")\n",
    "print(f\"Training F1 Score: {train_f1}\")\n",
    "print(f\"Training Confusion Matrix: \\n{train_conf_matrix}\")\n",
    "\n",
    "test_accuracy = accuracy_score(Y_test, test_predictions)\n",
    "test_precision = precision_score(Y_test, test_predictions)\n",
    "test_recall = recall_score(Y_test, test_predictions)\n",
    "test_f1 = f1_score(Y_test, test_predictions)\n",
    "test_conf_matrix = confusion_matrix(Y_test, test_predictions)\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")\n",
    "print(f\"Testing Precision: {test_precision}\")\n",
    "print(f\"Testing Recall: {test_recall}\")\n",
    "print(f\"Testing F1 Score: {test_f1}\")\n",
    "print(f\"Testing Confusion Matrix: \\n{test_conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
